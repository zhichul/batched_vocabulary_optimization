import numpy as np

a=model.bert.embeddings.word_embeddings.weight.detach().cpu().numpy()
a=a/np.linalg.norm(a, axis=-1)[:,None]
[(tokenizer.id2str(i), dp)for i, dp in sorted(enumerate(a @ a[85]), key=lambda x:x[1], reverse=True)[:10]]



 [(tokenizer.id2str(i), dp)for i, dp in sorted(enumerate(a @ a[85]), key=lambda x:x[1], reverse=True)[:10]]
[('@@ing', 1.0000001), ('@@ed', 0.29748386), ('@@ng', 0.29437745), ('@@ting', 0.29167956), ('@@es', 0.27748108), ('@@ned', 0.25189057), ('@@lion', 0.23730555), ('@@cts', 0.23567918), ('@@ies', 0.23276386), ('@@ding', 0.22966863)]
>>> [(tokenizer.id2str(i), dp)for i, dp in sorted(enumerate(a @ a[85]), key=lambda x:x[1], reverse=True)[:30]]
[('@@ing', 1.0000001), ('@@ed', 0.29748386), ('@@ng', 0.29437745), ('@@ting', 0.29167956), ('@@es', 0.27748108), ('@@ned', 0.25189057), ('@@lion', 0.23730555), ('@@cts', 0.23567918), ('@@ies', 0.23276386), ('@@ding', 0.22966863), ('@@ical', 0.22256187), ('@@ates', 0.22075975), ('services', 0.22055769), ('@@ion', 0.2179544), ('@@ade', 0.21696752), ('@@ial', 0.2151593), ('pa.', 0.21418917), ('@@ized', 0.21328358), ('@@uch', 0.21110539), ('@@red', 0.21056555), ('@@le', 0.20945928), ('@@ged', 0.20854047), ('@@e', 0.2084143), ('@@ble', 0.20725401), ('@@her', 0.20527874), ('@@se', 0.20525485), ('@@onal', 0.20524558), ('@@ide', 0.20466156), ('@@ted', 0.2024381), ('@@k>', 0.20146407)]
>>> [(tokenizer.id2str(i), dp)for i, dp in sorted(enumerate(a @ a[tokenizer.vocab_list.index("@@ed")]), key=lambda x:x[1], reverse=True)[:30]]
[('@@ed', 1.0000001), ('@@ted', 0.33584058), ('@@ing', 0.29748386), ('@@ated', 0.2791282), ('@@ned', 0.27142873), ('@@red', 0.26223475), ('issued', 0.24485934), ('fell', 0.23386472), ('@@ws', 0.22658697), ('rose', 0.22544143), ('won', 0.22486265), ('@@ged', 0.22392917), ('@@ts', 0.21929279), ('@@able', 0.2148017), ('made', 0.21464713), ('used', 0.21425416), ('@@sive', 0.20859274), ('led', 0.2083615), ('@@ized', 0.20787689), ('lost', 0.20449078), ('held', 0.20388383), ('possible', 0.2027775), ('@@ble', 0.20259118), ('@@duced', 0.20077221), ('up', 0.20018682), ('@@ded', 0.19948226), ('over', 0.19782835), ('@@e-a', 0.19665164), ('seen', 0.19615614), ('@@ates', 0.19574608)]
>>> [(tokenizer.id2str(i), dp)for i, dp in sorted(enumerate(a @ a[tokenizer.vocab_list.index("@@le")]), key=lambda x:x[1], reverse=True)[:30]]
[('@@le', 1.0), ('@@ion', 0.24118163), ('@@ble', 0.2359994), ('@@ng', 0.23065716), ('@@ting', 0.22266604), ('@@ne', 0.22246376), ('@@ge', 0.21085669), ('@@ing', 0.20945928), ('@@es', 0.20944953), ('@@ine', 0.20629297), ('@@ce', 0.20303664), ('@@age', 0.20246483), ('@@se', 0.202313), ('@@e', 0.19957219), ('@@nce', 0.1966323), ('@@ment', 0.19133186), ('@@ive', 0.18998548), ('graphics', 0.18395162), ('@@ity', 0.18207946), ('@@th', 0.17727605), ('@@tock', 0.17641607), ('late', 0.17525972), ('@@ian', 0.1750615), ('@@y', 0.17410776), ('heavy', 0.17392421), ('@@ry', 0.17312512), ('@@able', 0.17301935), ('[SP4]', 0.17142163), ('@@achinko', 0.1705039), ('@@est', 0.1698221)]
>>> [(tokenizer.id2str(i), dp)for i, dp in sorted(enumerate(a @ a[tokenizer.vocab_list.index("@@ng")]), key=lambda x:x[1], reverse=True)[:30]]
[('@@ng', 1.0), ('@@ing', 0.29437745), ('@@ting', 0.26755756), ('@@ance', 0.25835878), ('@@rlier', 0.2581597), ('@@nk>', 0.2576239), ('@@ding', 0.24669024), ('@@nce', 0.23414199), ('@@le', 0.23065716), ('and', 0.21855581), ('@@st', 0.21794395), ('@@-rate', 0.21708325), ('@@lar', 0.21663338), ('@@al', 0.21634895), ('@@se', 0.2145594), ('many', 0.2106578), ('@@able', 0.21021155), ('@@ce', 0.20958517), ('@@ine', 0.20508832), ('@@ment', 0.20508167), ('@@ght', 0.2047242), ('@@paper', 0.20434862), ('@@s', 0.2036793), ('@@d', 0.20113568), ('@@uch', 0.20078039), ('@@veral', 0.19838253), ('@@onal', 0.19758627), ('@@ular', 0.19735393), ('@@ally', 0.19695365), ('@@ion', 0.19588214)]
>>> [(tokenizer.id2str(i), dp)for i, dp in sorted(enumerate(a @ a[tokenizer.vocab_list.index("the")]), key=lambda x:x[1], reverse=True)[:30]]
[('the', 1.0), ('his', 0.28905416), ('their', 0.28064165), ('its', 0.26768282), ('an', 0.2618537), ('a', 0.25381196), ('@@ssive', 0.24663846), ('your', 0.24608475), ('our', 0.24577868), ('this', 0.24423622), ("'s", 0.24131316), ('german', 0.2380746), ('@@rly', 0.22270238), ('every', 0.22092041), ('whose', 0.21162671), ('another', 0.21137315), ('@@posed', 0.20820121), ('british', 0.2066561), ('dd', 0.20455684), ('mr.', 0.19911481), ('@@ritish', 0.19875225), ('my', 0.19825979), ('@@ious', 0.19689628), ('@@longed', 0.19586423), ('her', 0.19562146), ('@@other', 0.19553386), ('@@-tv', 0.19539258), ('@@urance', 0.1943932), ('huge', 0.19428359), ('each', 0.1942801)]
>>> [(tokenizer.id2str(i), dp)for i, dp in sorted(enumerate(a @ a[tokenizer.vocab_list.index("com")]), key=lambda x:x[1], reverse=True)[:30]]
[('com', 0.99999994), ('co', 0.26477203), ('@@-com', 0.2502287), ('re', 0.23362681), ('tem', 0.21251184), ('pro', 0.21202622), ('oper', 0.20772752), ('exam', 0.20373952), ('im', 0.2036326), ('ja', 0.20336066), ('ap', 0.20265006), ('cam', 0.20108616), ('su', 0.20018362), ('accom', 0.198432), ('instru', 0.1972039), ('cont', 0.19608772), ('sus', 0.19572227), ('ente', 0.1955997), ('ex', 0.1943288), ('seg', 0.19379506), ('att', 0.19290195), ('partic', 0.19091591), ('peo', 0.19011006), ('tele-com', 0.18864605), ('econo', 0.1875667), ('subcom', 0.1863803), ('funda', 0.18625739), ('@@robl', 0.18562862), ('@@rshi', 0.18518305), ('environ', 0.18465799)]
>>>