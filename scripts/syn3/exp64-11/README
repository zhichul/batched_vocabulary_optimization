small model sweep of E2E with default 0.02 lr on lattices and longer training epochs