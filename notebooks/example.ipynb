{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edc444ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_struct\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from typing import Dict\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce675838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch, N, C = 3, 7, 2\n",
    "def show_chain(chain):\n",
    "    plt.imshow(chain.detach().sum(-1).transpose(0, 1))\n",
    "\n",
    "# batch, N, z_n, z_n_1\n",
    "log_potentials = torch.rand(batch, N, C, C)\n",
    "dist = torch_struct.LinearChainCRF(log_potentials)\n",
    "show_chain(dist.argmax[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58017916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_chain(dist.marginals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86b3444f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "event = dist.to_event(torch.tensor([[0, 1, 0, 1, 1, 1, 0, 1]]), 2)\n",
    "show_chain(event[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e1d73",
   "metadata": {},
   "source": [
    "## Let's first write some code for converting a tokenization lattice to a linear chain CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f921698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.6931],\n",
      "        [1.0986],\n",
      "        [1.3863],\n",
      "        [1.6094]])\n"
     ]
    }
   ],
   "source": [
    "# first let's define a simple weighted vocabulary\n",
    "vocabulary = OrderedDict([(k,i) for i, k in enumerate([\"[PAD]\", \"h\", \"a\", \"t\", \"e\", \"at\", \"hat\", \"ate\", \"hate\"])])\n",
    "embedding = torch.nn.Embedding(num_embeddings=len(vocabulary), embedding_dim=1, padding_idx=0)\n",
    "weights = {\n",
    "    \"[PAD]\": 1.0,\n",
    "    \"h\": 1.0,\n",
    "    \"a\": 1.0,\n",
    "    \"t\": 1.0,\n",
    "    \"e\": 1.0,\n",
    "    \"at\": 2.0,\n",
    "    \"hat\": 3.0,\n",
    "    \"ate\": 4.0,\n",
    "    \"hate\": 5.0\n",
    "}\n",
    "for unit in weights.keys():\n",
    "    embedding.weight.data[vocabulary[unit]] = math.log(weights[unit])\n",
    "print(embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7c84547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]]])\n",
      "tensor([[[0.0000, 0.0000,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.0000, 0.0000,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf, 0.0000,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.0000, 0.0000,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.6931,   -inf, 0.0000,   -inf,   -inf,   -inf],\n",
      "         [1.0986,   -inf,   -inf, 0.0000,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.0000,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [1.3863,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [1.6094,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# now let's make the transition matrices\n",
    "def construct_chain(N: int, chunk: str, vocab: Dict[str, int], weights: torch.nn.Embedding, continuing_subword_prefix: str=None):\n",
    "    mask = torch.zeros((N, N, N), dtype=torch.float) # whenever a element of a matrix is from weights, set to 1\n",
    "    ids = torch.zeros((N, N, N), dtype=torch.int)\n",
    "    default_chain_weights = construct_default_chain_weights(N, len(chunk))\n",
    "    for s in range(len(chunk)):\n",
    "        for l in range(len(chunk) - s + 1):\n",
    "            unit = chunk[s:s + l]\n",
    "            unit = unit if continuing_subword_prefix is None or s == 0 else continuing_subword_prefix + unit\n",
    "            if unit in vocab:\n",
    "                mask[s+l-1, l-1, 0] = 1\n",
    "                ids[s+l-1, l-1, 0] = vocab[unit]\n",
    "    log_potentials = mask * weights(ids).squeeze(-1) + torch.log((1-mask) * default_chain_weights + mask)\n",
    "    return log_potentials\n",
    "\n",
    "def construct_default_chain_weights(N: int, L: int):\n",
    "    potentials = torch.zeros((N, N, N), dtype=torch.float) # fill with pad (id=0)\n",
    "    for i in range(L-1):\n",
    "        potentials[i, :i+1, 1:i+2] = torch.eye(i+1)\n",
    "    return potentials\n",
    "\n",
    "print(construct_default_chain_weights(6, 6))\n",
    "print(construct_chain(6, \"hate\", vocabulary, embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23bb1981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 20, 20, 20])\n",
      "tensor([ 5, 21, 21,  ..., 21, 21, 21], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "# now let's use torch-struct to compute log_potentials\n",
    "batch, N, C = 1024, 20, 20\n",
    "def show_chain(chain):\n",
    "    plt.imshow(chain.cpu().detach().sum(-1).transpose(0, 1))\n",
    "# batch, N, z_n, z_n_1\n",
    "log_potentials = construct_chain(N, \"hate\", vocabulary, embedding).unsqueeze(0).repeat(batch,1,1,1).transpose(-1, -2)\n",
    "print(log_potentials.size())\n",
    "lengths = torch.tensor([5] + [N+1] * (batch-1)).to(\"cuda:1\")\n",
    "print(lengths)\n",
    "# for i in tqdm(range(100)):\n",
    "#     dist = torch_struct.LinearChainCRF(log_potentials.to(\"cuda:1\"), lengths=lengths)\n",
    "#     dist.partition[0]\n",
    "#     # show_chain(dist.marginals[0])\n",
    "#     # show_chain(dist.argmax[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d6191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2598a5d",
   "metadata": {},
   "source": [
    "# Let's try to implement our own version using genbmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "768eecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import genbmm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fbc6529f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336453e49dfc4d18a1d3470ad0de0b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9459e+00],\n",
      "        [-1.0000e+09, -1.0000e+09,  6.9315e-01, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09,  1.3863e+00,  1.0986e+00],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  1.6094e+00]], device='cuda:1',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0986e+00],\n",
      "        [-1.0000e+09, -1.0000e+09,  6.9315e-01, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  1.0986e+00],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]], device='cuda:1',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]], device='cuda:1',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]], device='cuda:1',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([1600])\n"
     ]
    }
   ],
   "source": [
    "def construct_chain_mask(N: int, chunk: str, vocab: Dict[str, int], weights: torch.nn.Embedding, continuing_subword_prefix: str=None, reverse=False):\n",
    "    \"\"\"\n",
    "    reverse=False\n",
    "    [ h a  t   e   ]\n",
    "    [   ha at  te  ]\n",
    "    [      hat ate ]\n",
    "    [          hate]\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    reverse=True\n",
    "    first do this\n",
    "    [ h   a   t  e]\n",
    "    [ ha  at  te  ]\n",
    "    [ hat ate     ]\n",
    "    [ hate        ]\n",
    "    then flip left to right\n",
    "    \"\"\"\n",
    "    mask = torch.zeros((N, N), dtype=torch.float) # whenever a element of a matrix is from weights, set to 1\n",
    "    ids = torch.zeros((N, N), dtype=torch.int)\n",
    "    for s in range(len(chunk)):\n",
    "        for l in range(len(chunk) - s + 1):\n",
    "            unit = chunk[s:s + l]\n",
    "            unit = unit if continuing_subword_prefix is None or s == 0 else continuing_subword_prefix + unit\n",
    "            if unit in vocab:\n",
    "                if not reverse:\n",
    "                    mask[l-1, s+l-1] = 1\n",
    "                    ids[l-1, s+l-1] = vocab[unit]\n",
    "                else:\n",
    "                    mask[l-1, len(chunk)-s-1] = 1\n",
    "                    ids[l-1, len(chunk)-s-1] = vocab[unit]\n",
    "                    \n",
    "    return embedding(ids).squeeze(-1), mask\n",
    "\n",
    "\n",
    "def forward(edge_matrix, mask, lengths):\n",
    "    edge_log_potentials = torch.ones_like(edge_matrix) * (-1e9)\n",
    "    edge_log_potentials[mask.to(torch.bool)] = 0.0\n",
    "    edge_log_potentials += edge_matrix\n",
    "    \n",
    "    log_alphas = [mask.new_zeros(edge_matrix.size(0))]\n",
    "    n = edge_matrix.size(1)\n",
    "    bm = mask.to(torch.bool)\n",
    "    for i in range(n):\n",
    "        maski = (bm & torch.diag_embed(mask.new_ones(n-i, dtype=torch.bool), offset=i).unsqueeze(0)).to(torch.float)\n",
    "        update = log_alphas[-1][:,None, None] * maski\n",
    "        edge_log_potentials = edge_log_potentials +  update\n",
    "        log_alphas.append(torch.logsumexp(edge_log_potentials[:, :,i], -1))\n",
    "    return torch.gather(torch.stack(log_alphas), 0, lengths.unsqueeze(0)), edge_log_potentials #[log_alphas[l][i] for i, l in enumerate(lengths)]\n",
    "\n",
    "\n",
    "N=20\n",
    "b = 10\n",
    "gmask = torch.zeros(N,N,N, dtype=torch.float).to(\"cuda:1\")\n",
    "mmask = torch.zeros(N,N,N, dtype=torch.float).to(\"cuda:1\")\n",
    "triu_ones = torch.triu(torch.ones(N,N, dtype=torch.float).to(\"cuda:1\"), diagonal=0)\n",
    "for i in range(N):\n",
    "    gmask[i,:N-i,i:N] = triu_ones[:N-i, :N-i]\n",
    "    mmask[i,:N-i,i:N] = triu_ones[:N-i, :N-i]\n",
    "    gmask[i,0,:i] = 1\n",
    "    # gmask[i,:i,:i] = triu_ones[:i, :i]\n",
    "# print(gmask)\n",
    "for i in tqdm(range(10)):\n",
    "    edge_matrix_r, mask_r = construct_chain_mask(N, \"hate\", vocabulary, embedding, reverse=True)\n",
    "    edge_matrix_r = edge_matrix_r.unsqueeze(0).repeat(b, 1, 1).to(\"cuda:1\")\n",
    "    mask_r = mask_r.unsqueeze(0).repeat(b, 1, 1).to(\"cuda:1\")\n",
    "    edge_matrix_r = (edge_matrix_r.unsqueeze(0) * mmask.unsqueeze(1)).reshape(N * b, N, N)\n",
    "    mask_r = (mask_r.unsqueeze(0) * gmask.unsqueeze(1)).reshape(N * b, N, N)\n",
    "    lengths_r= torch.ones(N * b, dtype=torch.long).to(\"cuda:1\") * 4\n",
    "    Zbs, betas = forward(edge_matrix_r, mask_r, lengths_r)\n",
    "    if i == 0:\n",
    "        # print(betas[0])\n",
    "        print(eps[0][:4,:4])\n",
    "        print(eps[1][:4,:4])\n",
    "        print(eps[2][:4,:4])\n",
    "        print(eps[3][:4,:4])\n",
    "    edge_matrix, mask = construct_chain_mask(N, \"hate\", vocabulary, embedding)\n",
    "    edge_matrix = edge_matrix.unsqueeze(0).repeat(b, 1, 1).to(\"cuda:1\")\n",
    "    mask = mask.unsqueeze(0).repeat(b, 1, 1).to(\"cuda:1\")\n",
    "    lengths= torch.ones(b, dtype=torch.long).to(\"cuda:1\") * 4\n",
    "    Za, alpha = forward(edge_matrix, mask, lengths)\n",
    "    alpha = torch.masked_select(alpha, mask.to(torch.bool))\n",
    "    if i == 0:\n",
    "        betas = torch.masked_select(betas, mask.to(torch.bool).repeat_interleave(N, dim=0))\n",
    "        print(betas.size())\n",
    "    \n",
    "# del alphas\n",
    "# del betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0a97d72-bd60-41a3-918d-4a50236a0514",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'o' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m s\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag_embed(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat), offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mo\u001b[49m\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(t\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(s\u001b[38;5;241m.\u001b[39mgrad)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'o' is not defined"
     ]
    }
   ],
   "source": [
    "t = torch.diag_embed(torch.tensor([1,2,3], dtype=torch.float), offset=1)\n",
    "s = torch.diag_embed(torch.tensor([3,4,5], dtype=torch.float), offset=1)\n",
    "t.requires_grad_(True)\n",
    "s.requires_grad_(True)\n",
    "mask = torch.diag_embed(torch.tensor([1,1,1], dtype=torch.float), offset=1)\n",
    "o.sum().backward()\n",
    "print(t.grad)\n",
    "print(s.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c109bd-0cff-48ff-831d-a0dc42d558db",
   "metadata": {},
   "outputs": [],
   "source": [
    "math.e ** 1.7918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec5f2f-4cc6-4982-8ff5-e12213fe953b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl22]",
   "language": "python",
   "name": "conda-env-dl22-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
